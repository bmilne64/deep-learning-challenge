{"cells":[{"cell_type":"markdown","id":"55e4b4d4","metadata":{"id":"55e4b4d4"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":1,"id":"21b7f191","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21b7f191","executionInfo":{"status":"ok","timestamp":1676857781193,"user_tz":300,"elapsed":5676,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"12413e5e-ae71-4a99-8ad8-6cd5bd82c38b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive\n"]}],"source":["#import dependencies\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","import pandas as pd\n","import tensorflow as tf\n","from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive"]},{"cell_type":"code","execution_count":null,"id":"84b512f6","metadata":{"id":"84b512f6"},"outputs":[],"source":["#  Import and read the charity_data.csv.\n","# import pandas as pd \n","# application_df = pd.read_csv(\"Resources/charity_data.csv\")\n","# application_df.head()\n","\n","application_df = pd.read_csv('/gdrive/MyDrive/Colab/deep-learning-challenge/Resources/charity_data.csv')\n","application_df"]},{"cell_type":"code","execution_count":3,"id":"ca3d2e84","metadata":{"id":"ca3d2e84","executionInfo":{"status":"ok","timestamp":1676857781401,"user_tz":300,"elapsed":13,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"outputs":[],"source":["# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.\n","application_df = application_df.drop(['EIN', 'NAME'], axis=1)"]},{"cell_type":"code","execution_count":4,"id":"b74d70cc","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b74d70cc","executionInfo":{"status":"ok","timestamp":1676857781401,"user_tz":300,"elapsed":12,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"61b57c8f-812f-49b8-edf2-eac40d9739b9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["APPLICATION_TYPE            17\n","AFFILIATION                  6\n","CLASSIFICATION              71\n","USE_CASE                     5\n","ORGANIZATION                 4\n","STATUS                       2\n","INCOME_AMT                   9\n","SPECIAL_CONSIDERATIONS       2\n","ASK_AMT                   8747\n","IS_SUCCESSFUL                2\n","dtype: int64"]},"metadata":{},"execution_count":4}],"source":["# Determine the number of unique values in each column.\n","application_df.nunique()"]},{"cell_type":"code","execution_count":5,"id":"098f790d","metadata":{"id":"098f790d","executionInfo":{"status":"ok","timestamp":1676857781401,"user_tz":300,"elapsed":7,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"outputs":[],"source":["# Look at APPLICATION_TYPE value counts for binning\n","application_counts = application_df[\"APPLICATION_TYPE\"].value_counts()"]},{"cell_type":"code","execution_count":6,"id":"b6f59145","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b6f59145","executionInfo":{"status":"ok","timestamp":1676857781511,"user_tz":300,"elapsed":116,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"d489757d-1a44-44d6-c4cf-0c1ae4c3a833"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["T3       27037\n","T4        1542\n","T6        1216\n","T5        1173\n","T19       1065\n","T8         737\n","T7         725\n","T10        528\n","Other      276\n","Name: APPLICATION_TYPE, dtype: int64"]},"metadata":{},"execution_count":6}],"source":["# Choose a cutoff value and create a list of application types to be replaced\n","# use the variable name `application_types_to_replace`\n","application_types_to_replace = list(application_counts[application_counts<500].index)\n","\n","# Replace in dataframe\n","for app in application_types_to_replace:\n","    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,\"Other\")\n","\n","# Check to make sure binning was successful\n","application_df['APPLICATION_TYPE'].value_counts()"]},{"cell_type":"code","execution_count":7,"id":"ef63fc6e","metadata":{"id":"ef63fc6e","executionInfo":{"status":"ok","timestamp":1676857781511,"user_tz":300,"elapsed":2,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"outputs":[],"source":["# Look at CLASSIFICATION value counts for binning\n","classification_counts = application_df[\"CLASSIFICATION\"].value_counts()"]},{"cell_type":"code","execution_count":8,"id":"612c0d3c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"612c0d3c","executionInfo":{"status":"ok","timestamp":1676857781512,"user_tz":300,"elapsed":3,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"572db54e-f29a-425e-86ad-fa6e15b902fc"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["C1000    17326\n","C2000     6074\n","C1200     4837\n","C3000     1918\n","C2100     1883\n","C7000      777\n","C1700      287\n","C4000      194\n","C5000      116\n","C1270      114\n","C2700      104\n","C2800       95\n","C7100       75\n","C1300       58\n","C1280       50\n","C1230       36\n","C1400       34\n","C7200       32\n","C2300       32\n","C1240       30\n","C8000       20\n","C7120       18\n","C1500       16\n","C1800       15\n","C6000       15\n","C1250       14\n","C8200       11\n","C1238       10\n","C1278       10\n","C1235        9\n","C1237        9\n","C7210        7\n","C2400        6\n","C1720        6\n","C4100        6\n","C1257        5\n","C1600        5\n","C1260        3\n","C2710        3\n","C0           3\n","C3200        2\n","C1234        2\n","C1246        2\n","C1267        2\n","C1256        2\n","Name: CLASSIFICATION, dtype: int64"]},"metadata":{},"execution_count":8}],"source":["# You may find it helpful to look at CLASSIFICATION value counts >1\n","application_df[\"CLASSIFICATION\"].value_counts().loc[lambda x : x>1]"]},{"cell_type":"code","execution_count":9,"id":"bc6229a0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bc6229a0","executionInfo":{"status":"ok","timestamp":1676857781631,"user_tz":300,"elapsed":121,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"5b353bc4-8e79-479c-ff12-8db83f62278b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["C1000    17326\n","C2000     6074\n","C1200     4837\n","Other     2261\n","C3000     1918\n","C2100     1883\n","Name: CLASSIFICATION, dtype: int64"]},"metadata":{},"execution_count":9}],"source":["# Choose a cutoff value and create a list of classifications to be replaced\n","# use the variable name `classifications_to_replace`\n","classifications_to_replace = list(classification_counts[classification_counts<1000].index)\n","\n","\n","# Replace in dataframe\n","for cls in classifications_to_replace:\n","    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,\"Other\")\n","    \n","# Check to make sure binning was successful\n","application_df['CLASSIFICATION'].value_counts().loc[lambda x : x>1]"]},{"cell_type":"code","execution_count":10,"id":"9ac77390","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":317},"id":"9ac77390","executionInfo":{"status":"ok","timestamp":1676857781748,"user_tz":300,"elapsed":118,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"b6e92ee1-c2e6-45a6-958b-339bd3340ab5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   STATUS  ASK_AMT  IS_SUCCESSFUL  APPLICATION_TYPE_Other  \\\n","0       1     5000              1                       0   \n","1       1   108590              1                       0   \n","2       1     5000              0                       0   \n","3       1     6692              1                       0   \n","4       1   142590              1                       0   \n","\n","   APPLICATION_TYPE_T10  APPLICATION_TYPE_T19  APPLICATION_TYPE_T3  \\\n","0                     1                     0                    0   \n","1                     0                     0                    1   \n","2                     0                     0                    0   \n","3                     0                     0                    1   \n","4                     0                     0                    1   \n","\n","   APPLICATION_TYPE_T4  APPLICATION_TYPE_T5  APPLICATION_TYPE_T6  ...  \\\n","0                    0                    0                    0  ...   \n","1                    0                    0                    0  ...   \n","2                    0                    1                    0  ...   \n","3                    0                    0                    0  ...   \n","4                    0                    0                    0  ...   \n","\n","   INCOME_AMT_1-9999  INCOME_AMT_10000-24999  INCOME_AMT_100000-499999  \\\n","0                  0                       0                         0   \n","1                  1                       0                         0   \n","2                  0                       0                         0   \n","3                  0                       1                         0   \n","4                  0                       0                         1   \n","\n","   INCOME_AMT_10M-50M  INCOME_AMT_1M-5M  INCOME_AMT_25000-99999  \\\n","0                   0                 0                       0   \n","1                   0                 0                       0   \n","2                   0                 0                       0   \n","3                   0                 0                       0   \n","4                   0                 0                       0   \n","\n","   INCOME_AMT_50M+  INCOME_AMT_5M-10M  SPECIAL_CONSIDERATIONS_N  \\\n","0                0                  0                         1   \n","1                0                  0                         1   \n","2                0                  0                         1   \n","3                0                  0                         1   \n","4                0                  0                         1   \n","\n","   SPECIAL_CONSIDERATIONS_Y  \n","0                         0  \n","1                         0  \n","2                         0  \n","3                         0  \n","4                         0  \n","\n","[5 rows x 44 columns]"],"text/html":["\n","  <div id=\"df-c6b5bf83-424b-4dcc-9437-49278cc16806\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>STATUS</th>\n","      <th>ASK_AMT</th>\n","      <th>IS_SUCCESSFUL</th>\n","      <th>APPLICATION_TYPE_Other</th>\n","      <th>APPLICATION_TYPE_T10</th>\n","      <th>APPLICATION_TYPE_T19</th>\n","      <th>APPLICATION_TYPE_T3</th>\n","      <th>APPLICATION_TYPE_T4</th>\n","      <th>APPLICATION_TYPE_T5</th>\n","      <th>APPLICATION_TYPE_T6</th>\n","      <th>...</th>\n","      <th>INCOME_AMT_1-9999</th>\n","      <th>INCOME_AMT_10000-24999</th>\n","      <th>INCOME_AMT_100000-499999</th>\n","      <th>INCOME_AMT_10M-50M</th>\n","      <th>INCOME_AMT_1M-5M</th>\n","      <th>INCOME_AMT_25000-99999</th>\n","      <th>INCOME_AMT_50M+</th>\n","      <th>INCOME_AMT_5M-10M</th>\n","      <th>SPECIAL_CONSIDERATIONS_N</th>\n","      <th>SPECIAL_CONSIDERATIONS_Y</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>5000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>108590</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>5000</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>6692</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>142590</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 44 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6b5bf83-424b-4dcc-9437-49278cc16806')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c6b5bf83-424b-4dcc-9437-49278cc16806 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c6b5bf83-424b-4dcc-9437-49278cc16806');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}],"source":["# Convert categorical data to numeric with `pd.get_dummies`\n","app_dummies = pd.get_dummies(application_df)\n","app_dummies.head()"]},{"cell_type":"code","execution_count":11,"id":"5659bd80","metadata":{"id":"5659bd80","executionInfo":{"status":"ok","timestamp":1676857781748,"user_tz":300,"elapsed":2,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"outputs":[],"source":["# Split our preprocessed data into our features and target arrays\n","X = app_dummies.drop('IS_SUCCESSFUL', axis=1).values\n","y = app_dummies['IS_SUCCESSFUL'].values\n","\n","# Split the preprocessed data into a training and testing dataset\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)"]},{"cell_type":"code","execution_count":12,"id":"3b53b894","metadata":{"id":"3b53b894","executionInfo":{"status":"ok","timestamp":1676857781884,"user_tz":300,"elapsed":138,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"outputs":[],"source":["# Create a StandardScaler instances\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","\n","# Fit the StandardScaler\n","X_scaler = scaler.fit(X_train)\n","\n","# Scale the data\n","X_train_scaled = X_scaler.transform(X_train)\n","X_test_scaled = X_scaler.transform(X_test)"]},{"cell_type":"markdown","id":"31f6f53c","metadata":{"id":"31f6f53c"},"source":["## Compile, Train and Evaluate the Model"]},{"cell_type":"code","source":["# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n","input_features = X_train_scaled.shape[1]\n","input_features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lx3tErCu7jP2","executionInfo":{"status":"ok","timestamp":1676864806242,"user_tz":300,"elapsed":143,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"a4eb3af6-d03f-4b8c-f82e-0a581d68610c"},"id":"lx3tErCu7jP2","execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["43"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","execution_count":37,"id":"7307cf0a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7307cf0a","executionInfo":{"status":"ok","timestamp":1676865975624,"user_tz":300,"elapsed":122,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"d1ee06db-4c64-41b4-9b3e-58282c3c315f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_6\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_23 (Dense)            (None, 80)                3520      \n","                                                                 \n"," dense_24 (Dense)            (None, 50)                4050      \n","                                                                 \n"," dense_25 (Dense)            (None, 40)                2040      \n","                                                                 \n"," dense_26 (Dense)            (None, 1)                 41        \n","                                                                 \n","=================================================================\n","Total params: 9,651\n","Trainable params: 9,651\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["import tensorflow as tf\n","# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n","number_input_features = len(X_train[0])\n","\n","# X shape/ input feature/columns is 43 * 2/3 is 86-129 so chose 100 for the first hidden node\n","hidden_nodes_layer1 = 80\n","hidden_nodes_layer2 = 50\n","hidden_nodes_layer3 = 40\n","\n","nn = tf.keras.models.Sequential()\n","\n","# First hidden layer\n","nn.add(\n","    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",")\n","\n","# Second hidden layer\n","nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n","\n","# Third hidden layer\n","nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n","\n","# Output layer\n","nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n","\n","# Check the structure of the model\n","nn.summary()"]},{"cell_type":"code","execution_count":38,"id":"943352e2","metadata":{"id":"943352e2","executionInfo":{"status":"ok","timestamp":1676865980606,"user_tz":300,"elapsed":105,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"outputs":[],"source":["# Compile the model\n","nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"]},{"cell_type":"code","execution_count":39,"id":"88f0cd6a","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88f0cd6a","executionInfo":{"status":"ok","timestamp":1676866185751,"user_tz":300,"elapsed":203020,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"fdf6c9fc-bb6f-4fa3-8743-a4777b41da76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5688 - accuracy: 0.7241\n","Epoch 2/100\n","804/804 [==============================] - 1s 1ms/step - loss: 0.5545 - accuracy: 0.7313\n","Epoch 3/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5521 - accuracy: 0.7318\n","Epoch 4/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5499 - accuracy: 0.7324\n","Epoch 5/100\n","804/804 [==============================] - 1s 1ms/step - loss: 0.5492 - accuracy: 0.7345\n","Epoch 6/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5483 - accuracy: 0.7341\n","Epoch 7/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5472 - accuracy: 0.7344\n","Epoch 8/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5471 - accuracy: 0.7358\n","Epoch 9/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5459 - accuracy: 0.7360\n","Epoch 10/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5459 - accuracy: 0.7361\n","Epoch 11/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5453 - accuracy: 0.7358\n","Epoch 12/100\n","804/804 [==============================] - 1s 1ms/step - loss: 0.5449 - accuracy: 0.7367\n","Epoch 13/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5439 - accuracy: 0.7360\n","Epoch 14/100\n","804/804 [==============================] - 1s 1ms/step - loss: 0.5443 - accuracy: 0.7374\n","Epoch 15/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7374\n","Epoch 16/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5433 - accuracy: 0.7374\n","Epoch 17/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5432 - accuracy: 0.7385\n","Epoch 18/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5429 - accuracy: 0.7379\n","Epoch 19/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7385\n","Epoch 20/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7366\n","Epoch 21/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7380\n","Epoch 22/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7379\n","Epoch 23/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7387\n","Epoch 24/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5413 - accuracy: 0.7385\n","Epoch 25/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5415 - accuracy: 0.7382\n","Epoch 26/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5411 - accuracy: 0.7389\n","Epoch 27/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5409 - accuracy: 0.7382\n","Epoch 28/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7390\n","Epoch 29/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7400\n","Epoch 30/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7398\n","Epoch 31/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7392\n","Epoch 32/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5397 - accuracy: 0.7398\n","Epoch 33/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5402 - accuracy: 0.7386\n","Epoch 34/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5395 - accuracy: 0.7390\n","Epoch 35/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5395 - accuracy: 0.7399\n","Epoch 36/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7395\n","Epoch 37/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7399\n","Epoch 38/100\n","804/804 [==============================] - 1s 1ms/step - loss: 0.5391 - accuracy: 0.7400\n","Epoch 39/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7401\n","Epoch 40/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7398\n","Epoch 41/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5387 - accuracy: 0.7404\n","Epoch 42/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5385 - accuracy: 0.7403\n","Epoch 43/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7408\n","Epoch 44/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7399\n","Epoch 45/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5382 - accuracy: 0.7399\n","Epoch 46/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5383 - accuracy: 0.7402\n","Epoch 47/100\n","804/804 [==============================] - 1s 1ms/step - loss: 0.5379 - accuracy: 0.7408\n","Epoch 48/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5373 - accuracy: 0.7407\n","Epoch 49/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5379 - accuracy: 0.7408\n","Epoch 50/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5375 - accuracy: 0.7401\n","Epoch 51/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5373 - accuracy: 0.7407\n","Epoch 52/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5374 - accuracy: 0.7400\n","Epoch 53/100\n","804/804 [==============================] - 3s 3ms/step - loss: 0.5374 - accuracy: 0.7406\n","Epoch 54/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5370 - accuracy: 0.7409\n","Epoch 55/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5370 - accuracy: 0.7410\n","Epoch 56/100\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5371 - accuracy: 0.7406\n","Epoch 57/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5366 - accuracy: 0.7408\n","Epoch 58/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5370 - accuracy: 0.7397\n","Epoch 59/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5369 - accuracy: 0.7407\n","Epoch 60/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5365 - accuracy: 0.7407\n","Epoch 61/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5362 - accuracy: 0.7416\n","Epoch 62/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5362 - accuracy: 0.7410\n","Epoch 63/100\n","804/804 [==============================] - 1s 1ms/step - loss: 0.5361 - accuracy: 0.7409\n","Epoch 64/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5364 - accuracy: 0.7405\n","Epoch 65/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5361 - accuracy: 0.7410\n","Epoch 66/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5360 - accuracy: 0.7415\n","Epoch 67/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5362 - accuracy: 0.7417\n","Epoch 68/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5359 - accuracy: 0.7416\n","Epoch 69/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5362 - accuracy: 0.7412\n","Epoch 70/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5358 - accuracy: 0.7411\n","Epoch 71/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7409\n","Epoch 72/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5361 - accuracy: 0.7411\n","Epoch 73/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5358 - accuracy: 0.7416\n","Epoch 74/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5357 - accuracy: 0.7415\n","Epoch 75/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5354 - accuracy: 0.7408\n","Epoch 76/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5354 - accuracy: 0.7419\n","Epoch 77/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5354 - accuracy: 0.7414\n","Epoch 78/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5350 - accuracy: 0.7418\n","Epoch 79/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5353 - accuracy: 0.7416\n","Epoch 80/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5350 - accuracy: 0.7412\n","Epoch 81/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5354 - accuracy: 0.7409\n","Epoch 82/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5347 - accuracy: 0.7414\n","Epoch 83/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7418\n","Epoch 84/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5345 - accuracy: 0.7417\n","Epoch 85/100\n","804/804 [==============================] - 1s 1ms/step - loss: 0.5353 - accuracy: 0.7409\n","Epoch 86/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5346 - accuracy: 0.7413\n","Epoch 87/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5348 - accuracy: 0.7418\n","Epoch 88/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7418\n","Epoch 89/100\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5345 - accuracy: 0.7422\n","Epoch 90/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5349 - accuracy: 0.7418\n","Epoch 91/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5349 - accuracy: 0.7412\n","Epoch 92/100\n","804/804 [==============================] - 1s 1ms/step - loss: 0.5346 - accuracy: 0.7418\n","Epoch 93/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5346 - accuracy: 0.7417\n","Epoch 94/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5347 - accuracy: 0.7414\n","Epoch 95/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5345 - accuracy: 0.7412\n","Epoch 96/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5340 - accuracy: 0.7415\n","Epoch 97/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7416\n","Epoch 98/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7413\n","Epoch 99/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5342 - accuracy: 0.7415\n","Epoch 100/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5345 - accuracy: 0.7421\n"]}],"source":["# Train the model\n","fit_model = nn.fit(X_train_scaled,y_train,epochs=100)"]},{"cell_type":"code","execution_count":40,"id":"af66f3bd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"af66f3bd","executionInfo":{"status":"ok","timestamp":1676866576796,"user_tz":300,"elapsed":1183,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"a84f3bf5-0597-4877-93b3-4c08240a6bc0"},"outputs":[{"output_type":"stream","name":"stdout","text":["268/268 - 1s - loss: 0.5612 - accuracy: 0.7270 - 549ms/epoch - 2ms/step\n","Loss: 0.5611888766288757, Accuracy: 0.7269970774650574\n"]}],"source":["# Evaluate the model using the test data\n","model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n","print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"]},{"cell_type":"code","execution_count":58,"id":"bdb75f47","metadata":{"id":"bdb75f47","executionInfo":{"status":"ok","timestamp":1676868728777,"user_tz":300,"elapsed":140,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"outputs":[],"source":["# Export our model to HDF5 file\n","nn.save('/gdrive/MyDrive/Colab/deep-learning-challenge/Resources/AlphabetSoupCharity.h5')\n"]},{"cell_type":"code","source":["# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n","number_input_features = len(X_train[0])\n","hidden_nodes_layer1 = 100\n","hidden_nodes_layer2 = 80\n","hidden_nodes_layer3 = 60\n","hidden_nodes_layer4 = 40\n","\n","nn2 = tf.keras.models.Sequential()\n","\n","# First hidden layer\n","nn2.add(\n","    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",")\n","\n","# Second hidden layer\n","nn2.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n","\n","# third hidden layer\n","nn2.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n","\n","# Fourth hidden layer\n","nn2.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n","\n","# Output layer\n","nn2.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n","\n","# Check the structure of the model\n","nn2.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KTWlGo3_XvGa","executionInfo":{"status":"ok","timestamp":1676866579895,"user_tz":300,"elapsed":266,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"b5efcf59-ec04-4ddc-b184-0360bcc1cfef"},"id":"KTWlGo3_XvGa","execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_7\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_27 (Dense)            (None, 100)               4400      \n","                                                                 \n"," dense_28 (Dense)            (None, 80)                8080      \n","                                                                 \n"," dense_29 (Dense)            (None, 60)                4860      \n","                                                                 \n"," dense_30 (Dense)            (None, 40)                2440      \n","                                                                 \n"," dense_31 (Dense)            (None, 1)                 41        \n","                                                                 \n","=================================================================\n","Total params: 19,821\n","Trainable params: 19,821\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Compile the model\n","nn2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"metadata":{"id":"nCBu4SH7Z4AK","executionInfo":{"status":"ok","timestamp":1676866582581,"user_tz":300,"elapsed":111,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"id":"nCBu4SH7Z4AK","execution_count":42,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","fit_model = nn2.fit(X_train_scaled,y_train,epochs=100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wa6JXsbIZ4ZP","executionInfo":{"status":"ok","timestamp":1676866787072,"user_tz":300,"elapsed":203214,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"3048db87-7bb6-430d-9f1c-2ea80c13cb27"},"id":"Wa6JXsbIZ4ZP","execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5677 - accuracy: 0.7217\n","Epoch 2/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5555 - accuracy: 0.7306\n","Epoch 3/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5523 - accuracy: 0.7324\n","Epoch 4/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5504 - accuracy: 0.7327\n","Epoch 5/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5496 - accuracy: 0.7341\n","Epoch 6/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5488 - accuracy: 0.7341\n","Epoch 7/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5478 - accuracy: 0.7344\n","Epoch 8/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5469 - accuracy: 0.7356\n","Epoch 9/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5468 - accuracy: 0.7356\n","Epoch 10/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5461 - accuracy: 0.7358\n","Epoch 11/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5450 - accuracy: 0.7362\n","Epoch 12/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5446 - accuracy: 0.7360\n","Epoch 13/100\n","804/804 [==============================] - 3s 3ms/step - loss: 0.5445 - accuracy: 0.7368\n","Epoch 14/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5437 - accuracy: 0.7363\n","Epoch 15/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7379\n","Epoch 16/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.7371\n","Epoch 17/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5435 - accuracy: 0.7376\n","Epoch 18/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5426 - accuracy: 0.7378\n","Epoch 19/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5425 - accuracy: 0.7380\n","Epoch 20/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5419 - accuracy: 0.7379\n","Epoch 21/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5420 - accuracy: 0.7371\n","Epoch 22/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5412 - accuracy: 0.7382\n","Epoch 23/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5411 - accuracy: 0.7383\n","Epoch 24/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5409 - accuracy: 0.7392\n","Epoch 25/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5405 - accuracy: 0.7384\n","Epoch 26/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5402 - accuracy: 0.7386\n","Epoch 27/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5401 - accuracy: 0.7388\n","Epoch 28/100\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5398 - accuracy: 0.7389\n","Epoch 29/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5398 - accuracy: 0.7395\n","Epoch 30/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5399 - accuracy: 0.7395\n","Epoch 31/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5389 - accuracy: 0.7397\n","Epoch 32/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5388 - accuracy: 0.7395\n","Epoch 33/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5391 - accuracy: 0.7394\n","Epoch 34/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5387 - accuracy: 0.7395\n","Epoch 35/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5381 - accuracy: 0.7389\n","Epoch 36/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5383 - accuracy: 0.7397\n","Epoch 37/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5381 - accuracy: 0.7402\n","Epoch 38/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7390\n","Epoch 39/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5383 - accuracy: 0.7395\n","Epoch 40/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5376 - accuracy: 0.7398\n","Epoch 41/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5377 - accuracy: 0.7404\n","Epoch 42/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5374 - accuracy: 0.7404\n","Epoch 43/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5374 - accuracy: 0.7398\n","Epoch 44/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5371 - accuracy: 0.7403\n","Epoch 45/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5366 - accuracy: 0.7400\n","Epoch 46/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5372 - accuracy: 0.7405\n","Epoch 47/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5373 - accuracy: 0.7393\n","Epoch 48/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5365 - accuracy: 0.7405\n","Epoch 49/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5364 - accuracy: 0.7411\n","Epoch 50/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5361 - accuracy: 0.7413\n","Epoch 51/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5359 - accuracy: 0.7405\n","Epoch 52/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5361 - accuracy: 0.7404\n","Epoch 53/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5353 - accuracy: 0.7412\n","Epoch 54/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5370 - accuracy: 0.7411\n","Epoch 55/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5357 - accuracy: 0.7406\n","Epoch 56/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5356 - accuracy: 0.7407\n","Epoch 57/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5353 - accuracy: 0.7413\n","Epoch 58/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5352 - accuracy: 0.7415\n","Epoch 59/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5351 - accuracy: 0.7415\n","Epoch 60/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5354 - accuracy: 0.7410\n","Epoch 61/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5355 - accuracy: 0.7409\n","Epoch 62/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5354 - accuracy: 0.7407\n","Epoch 63/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7418\n","Epoch 64/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5348 - accuracy: 0.7412\n","Epoch 65/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5351 - accuracy: 0.7412\n","Epoch 66/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5345 - accuracy: 0.7413\n","Epoch 67/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5356 - accuracy: 0.7409\n","Epoch 68/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5345 - accuracy: 0.7411\n","Epoch 69/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5355 - accuracy: 0.7412\n","Epoch 70/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7416\n","Epoch 71/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7415\n","Epoch 72/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7418\n","Epoch 73/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5342 - accuracy: 0.7415\n","Epoch 74/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5342 - accuracy: 0.7418\n","Epoch 75/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7412\n","Epoch 76/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5342 - accuracy: 0.7420\n","Epoch 77/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5344 - accuracy: 0.7418\n","Epoch 78/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5340 - accuracy: 0.7416\n","Epoch 79/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5339 - accuracy: 0.7414\n","Epoch 80/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7416\n","Epoch 81/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7417\n","Epoch 82/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5344 - accuracy: 0.7416\n","Epoch 83/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7417\n","Epoch 84/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5330 - accuracy: 0.7419\n","Epoch 85/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5346 - accuracy: 0.7415\n","Epoch 86/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5345 - accuracy: 0.7416\n","Epoch 87/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5333 - accuracy: 0.7410\n","Epoch 88/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5335 - accuracy: 0.7416\n","Epoch 89/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5330 - accuracy: 0.7416\n","Epoch 90/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5339 - accuracy: 0.7411\n","Epoch 91/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5335 - accuracy: 0.7415\n","Epoch 92/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5330 - accuracy: 0.7418\n","Epoch 93/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5325 - accuracy: 0.7420\n","Epoch 94/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5325 - accuracy: 0.7421\n","Epoch 95/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5333 - accuracy: 0.7417\n","Epoch 96/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5328 - accuracy: 0.7411\n","Epoch 97/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7417\n","Epoch 98/100\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5329 - accuracy: 0.7416\n","Epoch 99/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5326 - accuracy: 0.7418\n","Epoch 100/100\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5327 - accuracy: 0.7423\n"]}]},{"cell_type":"code","source":["# Evaluate the model using the test data\n","model_loss, model_accuracy = nn2.evaluate(X_test_scaled,y_test,verbose=2)\n","print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bJAUCnP3aBbb","executionInfo":{"status":"ok","timestamp":1676866787711,"user_tz":300,"elapsed":643,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"ec4e73f1-5c77-4691-b443-984f178da01a"},"id":"bJAUCnP3aBbb","execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["268/268 - 0s - loss: 0.5786 - accuracy: 0.7249 - 460ms/epoch - 2ms/step\n","Loss: 0.5785872340202332, Accuracy: 0.7248979806900024\n"]}]},{"cell_type":"code","source":["# Export our model to HDF5 file\n","nn2.save('/gdrive/MyDrive/Colab/deep-learning-challenge/Resources/AlphabetSoupCharity_Optimization.h5')\n"],"metadata":{"id":"VGxFxojpF507","executionInfo":{"status":"ok","timestamp":1676868733765,"user_tz":300,"elapsed":124,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"id":"VGxFxojpF507","execution_count":59,"outputs":[]},{"cell_type":"code","source":["# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n","number_input_features = len(X_train[0])\n","hidden_nodes_layer1 = 100\n","hidden_nodes_layer2 = 80\n","hidden_nodes_layer3 = 60\n","hidden_nodes_layer4 = 40\n","\n","nn3 = tf.keras.models.Sequential()\n","\n","# First hidden layer\n","nn3.add(\n","    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",")\n","\n","# Second hidden layer\n","nn3.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n","\n","# third hidden layer\n","nn3.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n","\n","# Fourth hidden layer\n","nn3.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n","\n","# Output layer\n","nn3.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n","\n","# Check the structure of the model\n","nn3.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJYcusBAazbT","executionInfo":{"status":"ok","timestamp":1676867356427,"user_tz":300,"elapsed":158,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"5f675315-b230-407e-c6e2-2a3484e7649e"},"id":"lJYcusBAazbT","execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_10\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_42 (Dense)            (None, 100)               4400      \n","                                                                 \n"," dense_43 (Dense)            (None, 80)                8080      \n","                                                                 \n"," dense_44 (Dense)            (None, 60)                4860      \n","                                                                 \n"," dense_45 (Dense)            (None, 40)                2440      \n","                                                                 \n"," dense_46 (Dense)            (None, 1)                 41        \n","                                                                 \n","=================================================================\n","Total params: 19,821\n","Trainable params: 19,821\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Compile the model\n","nn3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"metadata":{"id":"6KyaWZ0BbCZA","executionInfo":{"status":"ok","timestamp":1676867360569,"user_tz":300,"elapsed":1019,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"id":"6KyaWZ0BbCZA","execution_count":51,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","fit_model = nn3.fit(X_train_scaled,y_train,epochs=200)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c7Kvz1IcbFyN","executionInfo":{"status":"ok","timestamp":1676867692785,"user_tz":300,"elapsed":330708,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"821ab7be-7eab-4d3a-978d-8b3646c430e5"},"id":"c7Kvz1IcbFyN","execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5676 - accuracy: 0.7234\n","Epoch 2/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5549 - accuracy: 0.7291\n","Epoch 3/200\n","804/804 [==============================] - 4s 5ms/step - loss: 0.5531 - accuracy: 0.7325\n","Epoch 4/200\n","804/804 [==============================] - 3s 3ms/step - loss: 0.5507 - accuracy: 0.7334\n","Epoch 5/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5495 - accuracy: 0.7344\n","Epoch 6/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5487 - accuracy: 0.7343\n","Epoch 7/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5474 - accuracy: 0.7346\n","Epoch 8/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5470 - accuracy: 0.7360\n","Epoch 9/200\n","804/804 [==============================] - 3s 4ms/step - loss: 0.5457 - accuracy: 0.7362\n","Epoch 10/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5457 - accuracy: 0.7356\n","Epoch 11/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5451 - accuracy: 0.7363\n","Epoch 12/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5446 - accuracy: 0.7373\n","Epoch 13/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5446 - accuracy: 0.7374\n","Epoch 14/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5436 - accuracy: 0.7375\n","Epoch 15/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5440 - accuracy: 0.7365\n","Epoch 16/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5434 - accuracy: 0.7373\n","Epoch 17/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5428 - accuracy: 0.7374\n","Epoch 18/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5423 - accuracy: 0.7379\n","Epoch 19/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5418 - accuracy: 0.7382\n","Epoch 20/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5418 - accuracy: 0.7377\n","Epoch 21/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5414 - accuracy: 0.7394\n","Epoch 22/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5414 - accuracy: 0.7389\n","Epoch 23/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5415 - accuracy: 0.7386\n","Epoch 24/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5399 - accuracy: 0.7399\n","Epoch 25/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5409 - accuracy: 0.7374\n","Epoch 26/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5401 - accuracy: 0.7388\n","Epoch 27/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5397 - accuracy: 0.7390\n","Epoch 28/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7393\n","Epoch 29/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5397 - accuracy: 0.7390\n","Epoch 30/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5400 - accuracy: 0.7388\n","Epoch 31/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5395 - accuracy: 0.7398\n","Epoch 32/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7394\n","Epoch 33/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5386 - accuracy: 0.7398\n","Epoch 34/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7397\n","Epoch 35/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5385 - accuracy: 0.7397\n","Epoch 36/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5390 - accuracy: 0.7398\n","Epoch 37/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5384 - accuracy: 0.7406\n","Epoch 38/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5379 - accuracy: 0.7401\n","Epoch 39/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5379 - accuracy: 0.7399\n","Epoch 40/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5378 - accuracy: 0.7401\n","Epoch 41/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5379 - accuracy: 0.7400\n","Epoch 42/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5376 - accuracy: 0.7404\n","Epoch 43/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5375 - accuracy: 0.7401\n","Epoch 44/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5373 - accuracy: 0.7404\n","Epoch 45/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5374 - accuracy: 0.7398\n","Epoch 46/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5371 - accuracy: 0.7405\n","Epoch 47/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5369 - accuracy: 0.7407\n","Epoch 48/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5370 - accuracy: 0.7406\n","Epoch 49/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5363 - accuracy: 0.7409\n","Epoch 50/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5370 - accuracy: 0.7408\n","Epoch 51/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5365 - accuracy: 0.7411\n","Epoch 52/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5365 - accuracy: 0.7409\n","Epoch 53/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5359 - accuracy: 0.7412\n","Epoch 54/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5359 - accuracy: 0.7415\n","Epoch 55/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5363 - accuracy: 0.7409\n","Epoch 56/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5360 - accuracy: 0.7418\n","Epoch 57/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5360 - accuracy: 0.7417\n","Epoch 58/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5358 - accuracy: 0.7404\n","Epoch 59/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5356 - accuracy: 0.7409\n","Epoch 60/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5357 - accuracy: 0.7406\n","Epoch 61/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5355 - accuracy: 0.7413\n","Epoch 62/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5357 - accuracy: 0.7414\n","Epoch 63/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5355 - accuracy: 0.7413\n","Epoch 64/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5350 - accuracy: 0.7406\n","Epoch 65/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5352 - accuracy: 0.7413\n","Epoch 66/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7413\n","Epoch 67/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5352 - accuracy: 0.7416\n","Epoch 68/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5349 - accuracy: 0.7415\n","Epoch 69/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5348 - accuracy: 0.7418\n","Epoch 70/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5353 - accuracy: 0.7416\n","Epoch 71/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5347 - accuracy: 0.7413\n","Epoch 72/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5349 - accuracy: 0.7417\n","Epoch 73/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5348 - accuracy: 0.7415\n","Epoch 74/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7417\n","Epoch 75/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7422\n","Epoch 76/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7418\n","Epoch 77/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5347 - accuracy: 0.7414\n","Epoch 78/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5352 - accuracy: 0.7417\n","Epoch 79/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5344 - accuracy: 0.7412\n","Epoch 80/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5342 - accuracy: 0.7410\n","Epoch 81/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5343 - accuracy: 0.7421\n","Epoch 82/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5339 - accuracy: 0.7410\n","Epoch 83/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5345 - accuracy: 0.7411\n","Epoch 84/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7418\n","Epoch 85/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5344 - accuracy: 0.7414\n","Epoch 86/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5345 - accuracy: 0.7410\n","Epoch 87/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5342 - accuracy: 0.7414\n","Epoch 88/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5340 - accuracy: 0.7419\n","Epoch 89/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7419\n","Epoch 90/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5351 - accuracy: 0.7415\n","Epoch 91/200\n","804/804 [==============================] - 3s 4ms/step - loss: 0.5336 - accuracy: 0.7419\n","Epoch 92/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5340 - accuracy: 0.7413\n","Epoch 93/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5331 - accuracy: 0.7421\n","Epoch 94/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5336 - accuracy: 0.7420\n","Epoch 95/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5333 - accuracy: 0.7422\n","Epoch 96/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5339 - accuracy: 0.7414\n","Epoch 97/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5361 - accuracy: 0.7411\n","Epoch 98/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5336 - accuracy: 0.7418\n","Epoch 99/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5329 - accuracy: 0.7420\n","Epoch 100/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5328 - accuracy: 0.7423\n","Epoch 101/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5331 - accuracy: 0.7416\n","Epoch 102/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5329 - accuracy: 0.7424\n","Epoch 103/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5331 - accuracy: 0.7411\n","Epoch 104/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5330 - accuracy: 0.7424\n","Epoch 105/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5330 - accuracy: 0.7421\n","Epoch 106/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5328 - accuracy: 0.7419\n","Epoch 107/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5330 - accuracy: 0.7428\n","Epoch 108/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5331 - accuracy: 0.7419\n","Epoch 109/200\n","804/804 [==============================] - 3s 4ms/step - loss: 0.5328 - accuracy: 0.7420\n","Epoch 110/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5328 - accuracy: 0.7426\n","Epoch 111/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5333 - accuracy: 0.7420\n","Epoch 112/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5335 - accuracy: 0.7420\n","Epoch 113/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5326 - accuracy: 0.7427\n","Epoch 114/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5325 - accuracy: 0.7425\n","Epoch 115/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5333 - accuracy: 0.7418\n","Epoch 116/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5351 - accuracy: 0.7421\n","Epoch 117/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5327 - accuracy: 0.7423\n","Epoch 118/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5324 - accuracy: 0.7420\n","Epoch 119/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7426\n","Epoch 120/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5324 - accuracy: 0.7420\n","Epoch 121/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5332 - accuracy: 0.7417\n","Epoch 122/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5324 - accuracy: 0.7424\n","Epoch 123/200\n","804/804 [==============================] - 3s 3ms/step - loss: 0.5326 - accuracy: 0.7425\n","Epoch 124/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5322 - accuracy: 0.7424\n","Epoch 125/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5323 - accuracy: 0.7421\n","Epoch 126/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5325 - accuracy: 0.7422\n","Epoch 127/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7421\n","Epoch 128/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5324 - accuracy: 0.7420\n","Epoch 129/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.7421\n","Epoch 130/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5323 - accuracy: 0.7414\n","Epoch 131/200\n","804/804 [==============================] - 3s 3ms/step - loss: 0.5329 - accuracy: 0.7423\n","Epoch 132/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5322 - accuracy: 0.7427\n","Epoch 133/200\n","804/804 [==============================] - 3s 4ms/step - loss: 0.5324 - accuracy: 0.7427\n","Epoch 134/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5323 - accuracy: 0.7424\n","Epoch 135/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5323 - accuracy: 0.7420\n","Epoch 136/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5319 - accuracy: 0.7423\n","Epoch 137/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5332 - accuracy: 0.7424\n","Epoch 138/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5320 - accuracy: 0.7424\n","Epoch 139/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5325 - accuracy: 0.7418\n","Epoch 140/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5338 - accuracy: 0.7420\n","Epoch 141/200\n","804/804 [==============================] - 3s 3ms/step - loss: 0.5321 - accuracy: 0.7417\n","Epoch 142/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5315 - accuracy: 0.7422\n","Epoch 143/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5321 - accuracy: 0.7426\n","Epoch 144/200\n","804/804 [==============================] - 3s 4ms/step - loss: 0.5316 - accuracy: 0.7423\n","Epoch 145/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5320 - accuracy: 0.7422\n","Epoch 146/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5324 - accuracy: 0.7420\n","Epoch 147/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5318 - accuracy: 0.7423\n","Epoch 148/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5327 - accuracy: 0.7418\n","Epoch 149/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5323 - accuracy: 0.7424\n","Epoch 150/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5335 - accuracy: 0.7415\n","Epoch 151/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5316 - accuracy: 0.7419\n","Epoch 152/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5311 - accuracy: 0.7423\n","Epoch 153/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5313 - accuracy: 0.7425\n","Epoch 154/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5315 - accuracy: 0.7421\n","Epoch 155/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5327 - accuracy: 0.7421\n","Epoch 156/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5318 - accuracy: 0.7426\n","Epoch 157/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5321 - accuracy: 0.7422\n","Epoch 158/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5316 - accuracy: 0.7427\n","Epoch 159/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5319 - accuracy: 0.7418\n","Epoch 160/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5316 - accuracy: 0.7421\n","Epoch 161/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5314 - accuracy: 0.7423\n","Epoch 162/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5333 - accuracy: 0.7428\n","Epoch 163/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5313 - accuracy: 0.7425\n","Epoch 164/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5362 - accuracy: 0.7414\n","Epoch 165/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5316 - accuracy: 0.7421\n","Epoch 166/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5315 - accuracy: 0.7426\n","Epoch 167/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5311 - accuracy: 0.7423\n","Epoch 168/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5312 - accuracy: 0.7421\n","Epoch 169/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5324 - accuracy: 0.7421\n","Epoch 170/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7418\n","Epoch 171/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5317 - accuracy: 0.7427\n","Epoch 172/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5314 - accuracy: 0.7427\n","Epoch 173/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5313 - accuracy: 0.7423\n","Epoch 174/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5313 - accuracy: 0.7421\n","Epoch 175/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5318 - accuracy: 0.7423\n","Epoch 176/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5322 - accuracy: 0.7421\n","Epoch 177/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7421\n","Epoch 178/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5313 - accuracy: 0.7425\n","Epoch 179/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5313 - accuracy: 0.7427\n","Epoch 180/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5315 - accuracy: 0.7427\n","Epoch 181/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5316 - accuracy: 0.7422\n","Epoch 182/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5312 - accuracy: 0.7428\n","Epoch 183/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5315 - accuracy: 0.7425\n","Epoch 184/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5380 - accuracy: 0.7421\n","Epoch 185/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5309 - accuracy: 0.7430\n","Epoch 186/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5313 - accuracy: 0.7423\n","Epoch 187/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5312 - accuracy: 0.7427\n","Epoch 188/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5316 - accuracy: 0.7427\n","Epoch 189/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5310 - accuracy: 0.7422\n","Epoch 190/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5319 - accuracy: 0.7424\n","Epoch 191/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5307 - accuracy: 0.7427\n","Epoch 192/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5311 - accuracy: 0.7427\n","Epoch 193/200\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5309 - accuracy: 0.7427\n","Epoch 194/200\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5318 - accuracy: 0.7425\n","Epoch 195/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5323 - accuracy: 0.7421\n","Epoch 196/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5314 - accuracy: 0.7420\n","Epoch 197/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5317 - accuracy: 0.7425\n","Epoch 198/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5305 - accuracy: 0.7427\n","Epoch 199/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5319 - accuracy: 0.7423\n","Epoch 200/200\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5318 - accuracy: 0.7416\n"]}]},{"cell_type":"code","source":["# Evaluate the model using the test data\n","model_loss, model_accuracy = nn3.evaluate(X_test_scaled,y_test,verbose=2)\n","print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Md1xP6V3bIz1","executionInfo":{"status":"ok","timestamp":1676867716977,"user_tz":300,"elapsed":607,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"1c28f28e-c543-4adb-9595-1aeb626afa9d"},"id":"Md1xP6V3bIz1","execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["268/268 - 0s - loss: 0.5782 - accuracy: 0.7245 - 451ms/epoch - 2ms/step\n","Loss: 0.5781909227371216, Accuracy: 0.7245481014251709\n"]}]},{"cell_type":"code","source":["# Export our model to HDF5 file\n","nn3.save('/gdrive/MyDrive/Colab/deep-learning-challenge/Resources/AlphabetSoupCharity_Optimization2.h5')"],"metadata":{"id":"qGbEUnEWGJJ9","executionInfo":{"status":"ok","timestamp":1676868740214,"user_tz":300,"elapsed":122,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"id":"qGbEUnEWGJJ9","execution_count":60,"outputs":[]},{"cell_type":"code","source":["# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.\n","number_input_features = len(X_train[0])\n","hidden_nodes_layer1 = 120\n","hidden_nodes_layer2 = 90\n","hidden_nodes_layer3 = 80\n","hidden_nodes_layer4 = 60\n","hidden_nodes_layer5 = 50\n","\n","nn4 = tf.keras.models.Sequential()\n","\n","# First hidden layer\n","nn4.add(\n","    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"tanh\")\n",")\n","\n","\n","# Second hidden layer\n","nn4.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n","\n","# third hidden layer\n","nn4.add(tf.keras.layers.Dense(units=hidden_nodes_layer3, activation=\"relu\"))\n","\n","# Fourth hidden layer\n","nn4.add(tf.keras.layers.Dense(units=hidden_nodes_layer4, activation=\"relu\"))\n","\n","# Fifth hidden layer\n","nn4.add(tf.keras.layers.Dense(units=hidden_nodes_layer5, activation=\"relu\"))\n","\n","# Output layer\n","nn4.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n","\n","# Check the structure of the model\n","nn4.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-rQqdtJbLiF","executionInfo":{"status":"ok","timestamp":1676867725229,"user_tz":300,"elapsed":258,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"1a740d30-6f03-4483-9c3d-6f09d8ae008e"},"id":"q-rQqdtJbLiF","execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_11\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," dense_47 (Dense)            (None, 120)               5280      \n","                                                                 \n"," dense_48 (Dense)            (None, 90)                10890     \n","                                                                 \n"," dense_49 (Dense)            (None, 80)                7280      \n","                                                                 \n"," dense_50 (Dense)            (None, 60)                4860      \n","                                                                 \n"," dense_51 (Dense)            (None, 50)                3050      \n","                                                                 \n"," dense_52 (Dense)            (None, 1)                 51        \n","                                                                 \n","=================================================================\n","Total params: 31,411\n","Trainable params: 31,411\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Compile the model\n","nn4.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"metadata":{"id":"tB8zzzVnEUk_","executionInfo":{"status":"ok","timestamp":1676867727611,"user_tz":300,"elapsed":109,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"id":"tB8zzzVnEUk_","execution_count":55,"outputs":[]},{"cell_type":"code","source":["# Train the model\n","fit_model = nn4.fit(X_train_scaled,y_train,epochs=250)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jsRL9pegEYYv","executionInfo":{"status":"ok","timestamp":1676868173186,"user_tz":300,"elapsed":444229,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"831633dc-5770-42bb-8e7b-5b93e9272b93"},"id":"jsRL9pegEYYv","execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/250\n","804/804 [==============================] - 3s 2ms/step - loss: 0.5656 - accuracy: 0.7246\n","Epoch 2/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5564 - accuracy: 0.7306\n","Epoch 3/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5535 - accuracy: 0.7320\n","Epoch 4/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5517 - accuracy: 0.7332\n","Epoch 5/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5513 - accuracy: 0.7335\n","Epoch 6/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5497 - accuracy: 0.7339\n","Epoch 7/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5489 - accuracy: 0.7346\n","Epoch 8/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5483 - accuracy: 0.7359\n","Epoch 9/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5468 - accuracy: 0.7376\n","Epoch 10/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5466 - accuracy: 0.7352\n","Epoch 11/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5468 - accuracy: 0.7357\n","Epoch 12/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5457 - accuracy: 0.7359\n","Epoch 13/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5445 - accuracy: 0.7377\n","Epoch 14/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5451 - accuracy: 0.7371\n","Epoch 15/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5433 - accuracy: 0.7376\n","Epoch 16/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5436 - accuracy: 0.7371\n","Epoch 17/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5427 - accuracy: 0.7386\n","Epoch 18/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5428 - accuracy: 0.7384\n","Epoch 19/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5427 - accuracy: 0.7379\n","Epoch 20/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5421 - accuracy: 0.7380\n","Epoch 21/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5422 - accuracy: 0.7378\n","Epoch 22/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5414 - accuracy: 0.7382\n","Epoch 23/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5412 - accuracy: 0.7388\n","Epoch 24/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5415 - accuracy: 0.7387\n","Epoch 25/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5412 - accuracy: 0.7381\n","Epoch 26/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5409 - accuracy: 0.7384\n","Epoch 27/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5402 - accuracy: 0.7397\n","Epoch 28/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5403 - accuracy: 0.7396\n","Epoch 29/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5395 - accuracy: 0.7406\n","Epoch 30/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5398 - accuracy: 0.7396\n","Epoch 31/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5392 - accuracy: 0.7402\n","Epoch 32/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5393 - accuracy: 0.7396\n","Epoch 33/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5389 - accuracy: 0.7399\n","Epoch 34/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5387 - accuracy: 0.7399\n","Epoch 35/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7403\n","Epoch 36/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5382 - accuracy: 0.7392\n","Epoch 37/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5376 - accuracy: 0.7402\n","Epoch 38/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5386 - accuracy: 0.7395\n","Epoch 39/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5385 - accuracy: 0.7394\n","Epoch 40/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5378 - accuracy: 0.7398\n","Epoch 41/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5379 - accuracy: 0.7397\n","Epoch 42/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5373 - accuracy: 0.7405\n","Epoch 43/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5377 - accuracy: 0.7399\n","Epoch 44/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5372 - accuracy: 0.7393\n","Epoch 45/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5374 - accuracy: 0.7407\n","Epoch 46/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5368 - accuracy: 0.7397\n","Epoch 47/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5369 - accuracy: 0.7401\n","Epoch 48/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5363 - accuracy: 0.7400\n","Epoch 49/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5365 - accuracy: 0.7401\n","Epoch 50/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5361 - accuracy: 0.7410\n","Epoch 51/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5369 - accuracy: 0.7403\n","Epoch 52/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5364 - accuracy: 0.7404\n","Epoch 53/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5360 - accuracy: 0.7408\n","Epoch 54/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5353 - accuracy: 0.7410\n","Epoch 55/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5362 - accuracy: 0.7411\n","Epoch 56/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5357 - accuracy: 0.7416\n","Epoch 57/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5359 - accuracy: 0.7409\n","Epoch 58/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5354 - accuracy: 0.7405\n","Epoch 59/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7417\n","Epoch 60/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7411\n","Epoch 61/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7410\n","Epoch 62/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5351 - accuracy: 0.7409\n","Epoch 63/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5341 - accuracy: 0.7414\n","Epoch 64/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5341 - accuracy: 0.7418\n","Epoch 65/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5354 - accuracy: 0.7412\n","Epoch 66/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7416\n","Epoch 67/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5342 - accuracy: 0.7415\n","Epoch 68/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5341 - accuracy: 0.7411\n","Epoch 69/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5347 - accuracy: 0.7402\n","Epoch 70/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7414\n","Epoch 71/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5342 - accuracy: 0.7409\n","Epoch 72/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5347 - accuracy: 0.7407\n","Epoch 73/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5339 - accuracy: 0.7411\n","Epoch 74/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5347 - accuracy: 0.7396\n","Epoch 75/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5342 - accuracy: 0.7414\n","Epoch 76/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5336 - accuracy: 0.7418\n","Epoch 77/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5346 - accuracy: 0.7412\n","Epoch 78/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5340 - accuracy: 0.7411\n","Epoch 79/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5335 - accuracy: 0.7414\n","Epoch 80/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5335 - accuracy: 0.7415\n","Epoch 81/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5375 - accuracy: 0.7387\n","Epoch 82/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5349 - accuracy: 0.7418\n","Epoch 83/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5335 - accuracy: 0.7414\n","Epoch 84/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5332 - accuracy: 0.7416\n","Epoch 85/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7414\n","Epoch 86/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5339 - accuracy: 0.7410\n","Epoch 87/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5337 - accuracy: 0.7408\n","Epoch 88/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7418\n","Epoch 89/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5337 - accuracy: 0.7418\n","Epoch 90/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5329 - accuracy: 0.7420\n","Epoch 91/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7415\n","Epoch 92/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5334 - accuracy: 0.7413\n","Epoch 93/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7417\n","Epoch 94/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5335 - accuracy: 0.7417\n","Epoch 95/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5334 - accuracy: 0.7415\n","Epoch 96/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7420\n","Epoch 97/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.7418\n","Epoch 98/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5333 - accuracy: 0.7414\n","Epoch 99/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5330 - accuracy: 0.7418\n","Epoch 100/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5343 - accuracy: 0.7421\n","Epoch 101/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5332 - accuracy: 0.7427\n","Epoch 102/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5328 - accuracy: 0.7418\n","Epoch 103/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7426\n","Epoch 104/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7425\n","Epoch 105/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5322 - accuracy: 0.7414\n","Epoch 106/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5336 - accuracy: 0.7415\n","Epoch 107/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5332 - accuracy: 0.7421\n","Epoch 108/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5322 - accuracy: 0.7423\n","Epoch 109/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5331 - accuracy: 0.7419\n","Epoch 110/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5346 - accuracy: 0.7411\n","Epoch 111/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7419\n","Epoch 112/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5332 - accuracy: 0.7425\n","Epoch 113/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5329 - accuracy: 0.7418\n","Epoch 114/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5324 - accuracy: 0.7415\n","Epoch 115/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5317 - accuracy: 0.7418\n","Epoch 116/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7424\n","Epoch 117/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5320 - accuracy: 0.7426\n","Epoch 118/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5317 - accuracy: 0.7424\n","Epoch 119/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7424\n","Epoch 120/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5323 - accuracy: 0.7420\n","Epoch 121/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5329 - accuracy: 0.7420\n","Epoch 122/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5328 - accuracy: 0.7419\n","Epoch 123/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5328 - accuracy: 0.7419\n","Epoch 124/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5318 - accuracy: 0.7426\n","Epoch 125/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5320 - accuracy: 0.7419\n","Epoch 126/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5316 - accuracy: 0.7422\n","Epoch 127/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7422\n","Epoch 128/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5338 - accuracy: 0.7423\n","Epoch 129/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7425\n","Epoch 130/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5316 - accuracy: 0.7426\n","Epoch 131/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5317 - accuracy: 0.7415\n","Epoch 132/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5313 - accuracy: 0.7422\n","Epoch 133/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5322 - accuracy: 0.7419\n","Epoch 134/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5323 - accuracy: 0.7423\n","Epoch 135/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5327 - accuracy: 0.7408\n","Epoch 136/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5329 - accuracy: 0.7422\n","Epoch 137/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5321 - accuracy: 0.7424\n","Epoch 138/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5318 - accuracy: 0.7422\n","Epoch 139/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5319 - accuracy: 0.7423\n","Epoch 140/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5315 - accuracy: 0.7420\n","Epoch 141/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5315 - accuracy: 0.7423\n","Epoch 142/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5315 - accuracy: 0.7426\n","Epoch 143/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5323 - accuracy: 0.7422\n","Epoch 144/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5333 - accuracy: 0.7416\n","Epoch 145/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5334 - accuracy: 0.7423\n","Epoch 146/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5319 - accuracy: 0.7423\n","Epoch 147/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5311 - accuracy: 0.7420\n","Epoch 148/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5316 - accuracy: 0.7427\n","Epoch 149/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7421\n","Epoch 150/250\n","804/804 [==============================] - 3s 3ms/step - loss: 0.5316 - accuracy: 0.7429\n","Epoch 151/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5312 - accuracy: 0.7420\n","Epoch 152/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5318 - accuracy: 0.7421\n","Epoch 153/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5313 - accuracy: 0.7420\n","Epoch 154/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5318 - accuracy: 0.7420\n","Epoch 155/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5318 - accuracy: 0.7423\n","Epoch 156/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5338 - accuracy: 0.7424\n","Epoch 157/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5318 - accuracy: 0.7416\n","Epoch 158/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5314 - accuracy: 0.7421\n","Epoch 159/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5312 - accuracy: 0.7427\n","Epoch 160/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5321 - accuracy: 0.7414\n","Epoch 161/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5315 - accuracy: 0.7424\n","Epoch 162/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5328 - accuracy: 0.7418\n","Epoch 163/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5309 - accuracy: 0.7425\n","Epoch 164/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5310 - accuracy: 0.7429\n","Epoch 165/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5324 - accuracy: 0.7421\n","Epoch 166/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5315 - accuracy: 0.7421\n","Epoch 167/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5311 - accuracy: 0.7426\n","Epoch 168/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5323 - accuracy: 0.7428\n","Epoch 169/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5318 - accuracy: 0.7416\n","Epoch 170/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5311 - accuracy: 0.7423\n","Epoch 171/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5314 - accuracy: 0.7423\n","Epoch 172/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5317 - accuracy: 0.7420\n","Epoch 173/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5313 - accuracy: 0.7422\n","Epoch 174/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5308 - accuracy: 0.7424\n","Epoch 175/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5309 - accuracy: 0.7420\n","Epoch 176/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5318 - accuracy: 0.7414\n","Epoch 177/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5316 - accuracy: 0.7420\n","Epoch 178/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5350 - accuracy: 0.7421\n","Epoch 179/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5310 - accuracy: 0.7427\n","Epoch 180/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5316 - accuracy: 0.7421\n","Epoch 181/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5315 - accuracy: 0.7407\n","Epoch 182/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5314 - accuracy: 0.7406\n","Epoch 183/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5313 - accuracy: 0.7407\n","Epoch 184/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5325 - accuracy: 0.7398\n","Epoch 185/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5318 - accuracy: 0.7402\n","Epoch 186/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5344 - accuracy: 0.7404\n","Epoch 187/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5320 - accuracy: 0.7411\n","Epoch 188/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5328 - accuracy: 0.7418\n","Epoch 189/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5316 - accuracy: 0.7412\n","Epoch 190/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5320 - accuracy: 0.7411\n","Epoch 191/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5314 - accuracy: 0.7419\n","Epoch 192/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5312 - accuracy: 0.7416\n","Epoch 193/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5311 - accuracy: 0.7424\n","Epoch 194/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5319 - accuracy: 0.7416\n","Epoch 195/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5323 - accuracy: 0.7414\n","Epoch 196/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5310 - accuracy: 0.7414\n","Epoch 197/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5315 - accuracy: 0.7424\n","Epoch 198/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5309 - accuracy: 0.7428\n","Epoch 199/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5310 - accuracy: 0.7433\n","Epoch 200/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5304 - accuracy: 0.7433\n","Epoch 201/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5309 - accuracy: 0.7424\n","Epoch 202/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5304 - accuracy: 0.7428\n","Epoch 203/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5306 - accuracy: 0.7430\n","Epoch 204/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5314 - accuracy: 0.7430\n","Epoch 205/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5311 - accuracy: 0.7419\n","Epoch 206/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5308 - accuracy: 0.7429\n","Epoch 207/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5307 - accuracy: 0.7429\n","Epoch 208/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5305 - accuracy: 0.7425\n","Epoch 209/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5307 - accuracy: 0.7428\n","Epoch 210/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5333 - accuracy: 0.7428\n","Epoch 211/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5333 - accuracy: 0.7423\n","Epoch 212/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5310 - accuracy: 0.7419\n","Epoch 213/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5306 - accuracy: 0.7429\n","Epoch 214/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5306 - accuracy: 0.7422\n","Epoch 215/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5303 - accuracy: 0.7421\n","Epoch 216/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5314 - accuracy: 0.7416\n","Epoch 217/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5319 - accuracy: 0.7418\n","Epoch 218/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5311 - accuracy: 0.7425\n","Epoch 219/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5306 - accuracy: 0.7424\n","Epoch 220/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5302 - accuracy: 0.7426\n","Epoch 221/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5309 - accuracy: 0.7421\n","Epoch 222/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5310 - accuracy: 0.7423\n","Epoch 223/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7419\n","Epoch 224/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5325 - accuracy: 0.7416\n","Epoch 225/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5307 - accuracy: 0.7421\n","Epoch 226/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5302 - accuracy: 0.7422\n","Epoch 227/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5302 - accuracy: 0.7426\n","Epoch 228/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5303 - accuracy: 0.7422\n","Epoch 229/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5312 - accuracy: 0.7431\n","Epoch 230/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5310 - accuracy: 0.7427\n","Epoch 231/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5321 - accuracy: 0.7428\n","Epoch 232/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5308 - accuracy: 0.7426\n","Epoch 233/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5309 - accuracy: 0.7427\n","Epoch 234/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5303 - accuracy: 0.7428\n","Epoch 235/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5305 - accuracy: 0.7425\n","Epoch 236/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5310 - accuracy: 0.7427\n","Epoch 237/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5311 - accuracy: 0.7422\n","Epoch 238/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5321 - accuracy: 0.7421\n","Epoch 239/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5303 - accuracy: 0.7420\n","Epoch 240/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5302 - accuracy: 0.7406\n","Epoch 241/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5307 - accuracy: 0.7428\n","Epoch 242/250\n","804/804 [==============================] - 1s 2ms/step - loss: 0.5316 - accuracy: 0.7422\n","Epoch 243/250\n","804/804 [==============================] - 2s 3ms/step - loss: 0.5308 - accuracy: 0.7421\n","Epoch 244/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5319 - accuracy: 0.7420\n","Epoch 245/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5326 - accuracy: 0.7409\n","Epoch 246/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5310 - accuracy: 0.7418\n","Epoch 247/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5299 - accuracy: 0.7420\n","Epoch 248/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5306 - accuracy: 0.7421\n","Epoch 249/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5306 - accuracy: 0.7423\n","Epoch 250/250\n","804/804 [==============================] - 2s 2ms/step - loss: 0.5307 - accuracy: 0.7424\n"]}]},{"cell_type":"code","source":["# Evaluate the model using the test data\n","model_loss, model_accuracy = nn4.evaluate(X_test_scaled,y_test,verbose=2)\n","print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XTV0qu5OEcP6","executionInfo":{"status":"ok","timestamp":1676868302500,"user_tz":300,"elapsed":616,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}},"outputId":"7e94838f-12b5-4bbd-b0fa-36149c243157"},"id":"XTV0qu5OEcP6","execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["268/268 - 0s - loss: 0.5852 - accuracy: 0.7259 - 443ms/epoch - 2ms/step\n","Loss: 0.5851919054985046, Accuracy: 0.7259474992752075\n"]}]},{"cell_type":"code","source":["# Export our model to HDF5 file\n","nn4.save('/gdrive/MyDrive/Colab/deep-learning-challenge/Resources/AlphabetSoupCharity_Optimization3.h5')"],"metadata":{"id":"xkqUCsHTGLqe","executionInfo":{"status":"ok","timestamp":1676868868192,"user_tz":300,"elapsed":1098,"user":{"displayName":"Bronwyn Milne","userId":"07106029233440226201"}}},"id":"xkqUCsHTGLqe","execution_count":63,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wGBzsvUpKnD0"},"id":"wGBzsvUpKnD0","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}